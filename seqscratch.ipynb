{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, MaxPool1D, Lambda, Concatenate, Input, Dropout, Input, LSTM\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('qndA.tsv', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sent = []\n",
    "fra_sent = []\n",
    "eng_chars = set()\n",
    "fra_chars = set()\n",
    "nb_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process english and french sentences\n",
    "for line in range(nb_samples):\n",
    "    \n",
    "    eng_line = str(lines[line]).split('\\t')[0]\n",
    "    \n",
    "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
    "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
    "    eng_sent.append(eng_line)\n",
    "    fra_sent.append(fra_line)\n",
    "    \n",
    "    for ch in eng_line:\n",
    "        if (ch not in eng_chars):\n",
    "            eng_chars.add(ch)\n",
    "            \n",
    "    for ch in fra_line:\n",
    "        if (ch not in fra_chars):\n",
    "            fra_chars.add(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_chars = sorted(list(fra_chars))\n",
    "eng_chars = sorted(list(eng_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each english character - key is index and value is english character\n",
    "eng_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get english character given its index - key is english character and value is index\n",
    "eng_char_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(eng_chars):\n",
    "    eng_index_to_char_dict[k] = v\n",
    "    eng_char_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each french character - key is index and value is french character\n",
    "fra_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get french character given its index - key is french character and value is index\n",
    "fra_char_to_index_dict = {}\n",
    "for k, v in enumerate(fra_chars):\n",
    "    fra_index_to_char_dict[k] = v\n",
    "    fra_char_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
    "max_len_fra_sent = max([len(line) for line in fra_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
    "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
    "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the english and french sentences\n",
    "\n",
    "for i in range(nb_samples):\n",
    "    for k,ch in enumerate(eng_sent[i]):\n",
    "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
    "        \n",
    "    for k,ch in enumerate(fra_sent[i]):\n",
    "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
    "\n",
    "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "        if k > 0:\n",
    "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoder model\n",
    "\n",
    "encoder_input = Input(shape=(None,len(eng_chars)))\n",
    "encoder_LSTM = LSTM(256,return_state = True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [encoder_h, encoder_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_input = Input(shape=(None,len(fra_chars)))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "#We train our model in two lines, while monitoring the loss on a held-out set of 20% of the samples.\n",
    "\n",
    "# Run training\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "# model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
    "#           y=target_data,\n",
    "#           batch_size=32,\n",
    "#           epochs=50,\n",
    "#           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('s2s.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load weights into new model\n",
    "model.load_weights(\"s2s.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "#To decode a test sentence, we will repeatedly:\n",
    "\n",
    "#1) Encode the input sentence and retrieve the initial decoder state\n",
    "#2) Run one step of the decoder with this initial state and a \"start of sequence\" token as target. The output will be the next target character.\n",
    "#3) Append the target character predicted and repeat.\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )\n",
    "\n",
    "#We use it to implement the inference loop described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
    "        translated_sent += sampled_fra_char\n",
    "        \n",
    "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return translated_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text CNN\n",
    "\n",
    "def textCNN(emb_dim, inp_shape):\n",
    "    inp = Input(inp_shape)\n",
    "    x = Conv2D(128, (1, emb_dim), activation='relu')(inp)\n",
    "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
    "    x = MaxPool1D(x.get_shape().as_list()[1])(x)\n",
    "    x = Lambda(lambda x: K.squeeze(x, axis=1))(x)\n",
    "\n",
    "    y = Conv2D(128, (1, emb_dim), activation='relu')(inp)\n",
    "    y = Lambda(lambda x: K.squeeze(x, axis=2))(y)\n",
    "    y = MaxPool1D(y.get_shape().as_list()[1])(y)\n",
    "    y = Lambda(lambda x: K.squeeze(x, axis=1))(y)\n",
    "\n",
    "    out = Concatenate(axis=1)([x, y])\n",
    "    return Model(inp, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "\n",
    "def discriminator(query_cnn, response_cnn):\n",
    "    query = Input(shape=(None, num_encoder_tokens, 1))\n",
    "    response = Input(shape=(None, latent_dim, 1))\n",
    "    \n",
    "    query_features = query_cnn(query) # [B, T, D] -> [B, all_features]\n",
    "    response_features = response_cnn(response)\n",
    "\n",
    "    feat = Concatenate(axis=1)([query_features, response_features])\n",
    "\n",
    "    x = Dense(128, activation='relu')(feat)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return Model([query, response], [out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineModel(gen, dis):\n",
    "    query = Input(shape=(None, num_encoder_tokens))\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    response = gen([query, decoder_inputs])\n",
    "    dis.trainable = False\n",
    "\n",
    "    feat, out = dis([query, response])\n",
    "    return Model(query, [feat, out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 188\n",
    "input_shape = (8, latent_dim, 1)\n",
    "query_cnn = textCNN(latent_dim, input_shape)\n",
    "response_cnn = textCNN(latent_dim, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = 188\n",
    "num_decoder_tokens = 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 188, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None, 188, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Model)                 (None, 256)          48384       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Model)                 (None, 256)          48384       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           model_4[1][0]                    \n",
      "                                                                 model_5[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          33024       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            257         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 195,713\n",
      "Trainable params: 195,713\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dis = discriminator(query_cnn, response_cnn)\n",
    "dis.compile(loss = 'binary_crossentropy', optimizer= Adam(5e-4), metrics=['accuracy'])\n",
    "dis.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = combineModel(dis, model)\n",
    "combined_model.compile(loss = ['mae', 'binary_crossentropy'], optimizer=Adam(5e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((batch_size, 1))\n",
    "zeros = np.zeros((batch_size, 1))\n",
    "\n",
    "for e in range(epoch):\n",
    "    avgL1, avgL2, avgL3 = 0, 0, 0\n",
    "    for i ,query, dinput_data, response in enumerate(zip(encoder_input_data, decoder_input_data, decoder_target_data)):\n",
    "\n",
    "        #data_gen function to get query and its corresoponding response\n",
    "        #query, real_response = next(data_gen(trainImages, imageDir, annotationDir, batch_size))\n",
    "        \n",
    "        #Generate fake response from the generator\n",
    "        fake_response = gen.predict(query, dinput_data)\n",
    "\n",
    "        #get feature vector from the discriminator for both real and fake response\n",
    "        feat1, _ = dis([query, real_responses])\n",
    "        feat2, _ = dis([query, fake_responses])\n",
    "\n",
    "        #Train the discriminator\n",
    "        dis_loss_1 = dis.train_on_batch([query, real_responses], ones)\n",
    "        dis_loss_2 = dis.train_on_batch([query, fake_responses], zeros)\n",
    "\n",
    "        #Train the generator\n",
    "        cgan_loss = combined_model.train_on_batch(query, [feat1, ones])\n",
    "        \n",
    "        #Add discriminator loss\n",
    "        dis_loss = 0.5 * np.add(dis_loss_1, dis_loss_2)\n",
    "        loss_1.append(dis_loss)\n",
    "        # avgL1 += dis_loss[0]\n",
    "\n",
    "        #Feature Loss\n",
    "        loss_2.append(cgan_loss)\n",
    "        # avgL2 += cgan_loss[1]\n",
    "\n",
    "        if((i+1)%5 == 0):\n",
    "            print(\"Epoch %d/%d   iteration %d/%d  D-Acc %3d%%  D-Loss: %f  cGAN_Dis-Loss: %f\" % (e+1, epoch, i+1, iterations, 100*dis_loss[1], dis_loss[0], cgan_loss[0]))\n",
    "        \n",
    "        if(i == iterations-1):\n",
    "            break\n",
    "        \n",
    "    # loss_3.append([avgL1/iterations, avgL2/iterations])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: anybody else seeing fb corruption during boot?\n",
      "Decoded sentence: i have a lisk and whet is the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the problem in the install the \n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2,3):\n",
    "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', eng_sent[seq_index])\n",
    "    print('Decoded sentence:', translated_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
